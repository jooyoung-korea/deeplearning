{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 강화학습 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습과 관련된 수식 속 함축적인 의미에 대해 잘 알지 못하면 강화학습을 직관적으로 이해하기에 한계가 있다.<br /> **강화학습은 에이전트가 의사 결정을 위해 사용하는 프레임워크다.** 구체화된 에이전트가 어쩌면 강화학습을 완전히 인식하고 활용하는 가장 좋은 방법일 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에이전트는 **환경(environment)** 내에 위치. 그 환경에는 부분적으로 혹은 완전히 관측 가능한 **상태(state)** 가 있다. 에이전트는 자신의 환경과 상호작용하기 위해 사용할 수 있는 **행동(action)** 집합을 갖는다. 행동의 결과는 그 환경을 새로운 상태로 전이시킨다. 에이전트는 행동을 실행한 후 그에 대응하는 스칼라 **보상(reward)** 를 받는다. 에이전트의 목적은 어떤 상태가 주어졌을 때 어떤 행동을 취할지 결정할 **정책(policy)** 을 학습함으로써 미래에 받을 누적된 보상을 최대화하는 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화의 핵심은 바로 보상을 얻게 해주는 행동의 빈도 증가.<br />\n",
    "강화는 행동심리학에서 연구됐던 분야지만 사실 우리에게 친숙한 개념입니다. 아이가 첫걸음을 떼는 과정에서\n",
    "아이는 지금까지 누구에게 걷는 것을 배운 적이 없지만 스스로 이것저것 시도해 보면서 걷습니다. 그러다 우연히 걷게 되면\n",
    "자신이 하는 행동과 걷게  된다는 보상 사이의 연관관계를 몰라 다시 넘어집니다. 하지만 시간이 지남에 따라 그 관계를 잘 학습해서 결국 잘 걷습니다. \n",
    "이렇게 사람과 동물에게 학습의 기본이 되는 강화라는 개념이 강화학습의 모티프가 된 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 강화학습 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습은 결정을 순차적으로 내려야 하는 문제에 적용됩니다. 순차적으로 행동을 결정하는 문제를 정의할 때 사용하는 방법이 MDP(Markov Decision Process)입니다. MDP는 순차적 행동 결정 문제를 수학적으로 정의해서 에이전트가 순차적 행동 결정 문제에 접근할 수 있게 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "순차적 행동 결정 문제의 구성 요소\n",
    "+ 상태\n",
    "    - _에이전트의 상태, 정적인 요소 뿐만 아니라 에이전트가 움직이는 속도와 같은 동적인 요소도 상태에 포함_\n",
    "+ 행동\n",
    "    + _에이전트가 어떠한 상태에서 취할 수 있는 행동, 에이전트가 행동을 취하면 환경은 에이전트에게 보상을 주고 다음 상태를 알려줌._\n",
    "+ 보상\n",
    "    + _가장 핵심적인 요소, 강화학습의 목표는 시간에 따라 얻는 보상들의 합을 최대로 하는 정책을 찾는 것, 보상은 에이전트에 속하지 않는 환경의 일부이고 에이전트는 어떤 상황에서 얼마의 보상이 나오는 지 미리 알지 못함._\n",
    "+ 정책\n",
    "    + _제일 좋은 정책은 최적 정책(optimal policy)라고 하며 이 때 보상의 합을 최대로 받을 수 있다. 정책이란 순차적 행동 결정 문제의 처음 부터 끝까지 모든 상태에 대한 에이전트의 행동을 말함._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP와 벨만 방정식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP는 _**상태, 행동, 보상함수, 상태 변환 환률(State Transition Probability), 감가율(Discount Factor)**_ 로 구성돼 있습니다.<br />\n",
    "문제의 정의는 에이전트가 학습하는 데 가장 중요한 단계 중 하나 입니다. 순차적으로 행동을 결정하는 문제에 대한 정의가 바로 MDP입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**상태 변환 확률** 은 상태 s에서 행동 a를 취했을 때 다른 상태 s'에 도달할 확률입니다. 이 값은 보상과 마찬가지로 에이전트가 알지 못하는 값으로서 에이전트가 아닌 환경의 일부. 상태 변환 확률은 환경의 모델이라고도 부름. 환경은 에이전트가 행동을 취하면 상태 변환 확률을 통해 다음에 에이전트가 갈 상태를 알려줌."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**감가율**은 시간에 따라서 감가하는 비율을 말하며 $ \\gamma $ 로 표기함. 감가율 $ \\gamma $ 는 0과 1 사이의 값! 따라서 보상에 곱해지면 보상이 감소. 만약 현재의 시간 t로 시간 k가 지난 후에 보상을 $ R_{t+k} $ 를 받을 것이라고 하면 그 보상의 가치는 현재로부터 시간이 k만큼 지났기 때문에 미래에 받을 보상 $ R_{t+k} $ 는 $ \\gamma^{k-1} $ 만큼 감가됨.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**정책**은 모든 상태에서 에이전트가 할 행동. 상태가 입력으로 들어오면 행동을 출력으로 내보내는 일종의 함수라고 생각가능<br />\n",
    "$ \\pi(a|s) = P[A_t = a|S_t = s] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가치함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 에이전트가 학습할 수 있도록 문제를 MDP로 정의했다. 에이전트는 MDP를 통해 최적 정책을 찾으면 된다. 어떻게 찾을 수 있을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 반환값의 정의(감가율을 적용한 보상들의 합)<br />\n",
    "$ G_b = R_{t+1} + \\gamma R_{t+2} + \\gamma^2R_{t+3} + \\gamma^3R_{t+4} ... $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 가치함수<br />\n",
    "$ v(s) = E[G_t | S_t = s] $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 앞으로 받을 보상에 대한 기댓값인 가치함수 <br />\n",
    "$ v(s) = E[R_{t+1} + \\gamma R_{t+2} +\\gamma^2R_{t+3} ... | S_t = s] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 반환값으로 나타내는 가치함수<br />\n",
    "$ v(s) = E[R_{t+1} + \\gamma G_{t+1} | S_t = s] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 가치함수로 표현하는 가치함수의 정의<br />\n",
    "$ v(s) = E[R_{t+1} + \\gamma v(S_{t+1})|S_t = s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 정책을 고려한 가치함수의 표현 **(벨만 기대 방정식<sup> Bellman Expectation Equation</sup>)** <br />\n",
    "$ v_{\\pi} = E_{\\pi}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1})|S_t = s] $ <br /><br />\n",
    "벨만 기대 방정식은 강화학습에서 상당히 중요. 현재 상태의 가치함수$(v_{\\pi}(s))$와 다음 상태의 가치함수$(v_{\\pi}(s+1))$사이의 관계를 말해주는 방정식. 강화학습은 벨만 방정식을 어떻게 풀어나가느냐의 스토리!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 중요한 질문은 RL문제가 ${\\pi}^{*}$ 를 구하는 것이라면 에이전트가 환경과 상호작용하면서 어떻게 학습하는가 하는 것이다. 아래 방정식은 반환 값을 계산하기 위해 시도할 행동과 다음 상태를 명시적으로 가리키지 않는다. RL에서는 Q값을 사용하면 ${\\pi}^{*}$ 를 학습하기가 더 쉽다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${\\pi}^{*} = argmax_a Q(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "즉 모든 상태에 대해 값을 최대화하는 정책을 찾기보다 모든 상태에 대해 품질(Q)값을 최대화하는 행동을 찾는다. Q가치함수를 구했으면 $V^* 와 {\\pi}^*$ 는 구할 수 있다.<br />\n",
    "모든 행동에 대해 보상과 다음 상태를 관측할 수 있다면 Q 값을 학습하기 위해 다음처럼 반복적인 시행착오 알고리즘을 공식화할 수 있다.<br />\n",
    "$ Q(s,a) = r + r max_{a^{'}}Q(s^{'},a{'}) $ <br />\n",
    "$s^{'},a{'}$ 는 각각 다음 상태와 행동을 말한다. 위 방정식은 Q-러닝 알고리즘의 핵심인 **벨만 방정식** 이라고 한다.<br/>\n",
    "에이전트는 환경의 역학에 대해 모른 채로 행동 a를 시도하고 보상 r과 다음 상태 $s^{'}$ 형태로 어떤 일이 일어나는지 관측한다. $max_{a^{'}}Q(s^{'},a{'})$ 는 다음 상태에 대해 최대 Q값을 주는 논리적 행동을 선택한다. 방정식의 모든 항을 알고 있으면 그 현재 상태-행동 쌍을 위한 Q값이 업데이트된다. 반복적으로 업데이트를 수행하다 보면 결국 Q함수를 학습할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-러닝은 'off-policy' RL알고리즘이다. 이 알고리즘은 해당 정책으로부터 경험을 직접 샘플링하지 않고도 정책 개선을 위해 학습한다. 즉, Q값은 에이전트에 의해 사용된 기반 정책과 독립적으로 학습 된다 <br />\n",
    "* on policy: 지금행동으로수집한데이터를이용해서실시간으로갱신. policygradient에서사용되는방법. \n",
    "* off policy: 지금행동으로수집한데이터를갱신하는데쓰지않는다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습은 수많은 게임 문제를 해결할 수 있지만, Q-테이블로는 더 복잡한 실세계 문제로 확장할 수 없다. 이 문제는 심층 신경망을 이용해 Q-테이블을 학습해 해결할 수 있다. 그렇지만 강화학습에서 심층 신경망을 훈련시키는 것은 샘플 간 상관관계와 Q-네트워크의 유동성 때문에 매우 불안정하다. <br />\n",
    "DQN은 이 문제의 해결책으로 경험 재구현을 사용하고 훈련 시 타깃 네트워크를 Q-네트워크와 분리할 것을 제안한다. DQN을 개선하는 다른 모델들도 있다. 우선 경험 재구현은 경험 버퍼가 균등하게 샘플링돼서는 안 된다는 것을 보여준다. 대신 TD(시간차 학습) 오차에 기반한 더 중요한 경험은 훈련 효율성을 높이기 위해 더 자주 샘플링 되어야 한다. <br />\n",
    "[Tom Schaul and others, Prioritized experience replay]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
