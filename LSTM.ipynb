{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Architecture\n",
    "In order to study the architecture of an LSTM, let's quickly recall the architecture of an RNN. Basically what we do is we take our event $E_t$ and our memory $M_{t-1}$, coming from the previous point in time,and we apply a simple $\\tan h$ or sigmoid activation function to obtain the output and memory $M_t$<br /><br />$M_t = \\tan h(W[STM_{t-1},E_t] + b)$<br /><br />We join these two vectors($STM_{t-1},E_t$) and multiply them by a matrix $W$ and add a bias $b$, and then squish this with the tanh function and that gives us the output $M_t$. This output is a prediction and also the memory that we carry to the next node.<br /> The LSTM architecture is very similar, except with a lot more nodes inside and with two inputs and outputs since it keeps track of the long and short term memories.\n",
    "![image](https://user-images.githubusercontent.com/46597294/72408943-b2612b00-37a7-11ea-9134-5086051f7dc4.png)\n",
    "<br />\n",
    "## Learn Gate\n",
    "What the learn gate does is following: it takes a **short term memory** and **the event** and it joins it. Actually, it does a bit more. **It takes the short term memory and the event and it combines them and then it ignores a bit of it keeping the important part of it.** <br /><br />\n",
    "* And how does this work mathematically?<br /><br />\n",
    "We have the short term memory $STM_{t-1}$ and the event $E_t$ and it combines them by putting them through a linear function which consists of joining the vectors multiplying by a matrix $W_n$ adding a bias $b_n$ and finally squishing the result with a $\\tan h$ activation function. <br /><br />\n",
    "* Now, how do we ignore part of it?<br /><br /> By multiplying an ignore factor $i_t$ which is actually a vector but it multiplies element wise. <br /><br />\n",
    "* And how do we calculate $i_t$? <br /><br />Well, we use our previous information of short term memory and the event. So again, we create a small neural network whose inputs are the short term memory and the event. We'll pass them through a small linear function with a new matrix and a new bias and squish them with the sigmoid function to keep it between 0 and 1. That's how the learn gate works!<br />\n",
    "![image](https://user-images.githubusercontent.com/46597294/72409473-3bc52d00-37a9-11ea-84cc-6dbf258f534f.png)\n",
    "\n",
    "## Forget Gate\n",
    "\n",
    "This one works as follows: It takes a **long term memory** and **it decides what parts to keep and to forget**.<br /><br />\n",
    "* How does the Forget Gate work mathematically?<br /><br />\n",
    "The long term memory($LTM_{t-1}$) from time $t-1$ comes in,and it gets multiplied by a forget factor $f_t$.<br /><br />\n",
    "* How does the forget factor $f_t$ get calculated?<br /><br />\n",
    "We'll use a short term memory($STM_{t-1}$) and the event information($E_t$) to calculate $f_t$. So just as before, we run a small layer of neural network with a linear function combined with the sigmoid function to calculate this Forget Factor and that's how the Forget Gate works.<br />\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/46597294/72410130-028dbc80-37ab-11ea-94c8-b20cffe46300.png)\n",
    "\n",
    "## Remember Gate\n",
    "It takes the long term memory coming out of the Forget Gate and the short term memory coming out of the Learn Gate and simply combines them together.<br /><br />\n",
    "* How does this work mathematically?<br /><br />\n",
    "We just take the outputs coming from the Forget Gate and from the Learn Gate and we just add them. And that's all we do! \n",
    "\n",
    "![image](https://user-images.githubusercontent.com/46597294/72410360-9bbcd300-37ab-11ea-8f0f-392d5d482acb.png)\n",
    "\n",
    "## Use Gate\n",
    "Use Gate(or output gate), this is the one that **uses the long term memory that just came out of the forget gate and the short term memory just came out of the learn gate, to come up with a new short term memory and an output.** These are the same thing. In this case, we'll take what's useful from the long term memory and  what's useful from the short term memory, and that's what's going to be our new short term memory. <br />\n",
    "* Mathematically what this does is the following:<br /><br />It applies a small neural network on the output of the forget gate using the $\\tan h$ activation function,$U_t = \\tan h(W_uLTM_{t-1}\\cdot f_t + b_u)$ and it applies to another small neural network on the short term memory and the events using the sigmoid activation function. $V_t = \\sigma(W_v[STM_{t-1},E_t]+b_v)$ As a final steps, it multiplies these two in order to get the new output. $STM_t = U_t\\cdot V_t$The output also worth of the new short term memory.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/46597294/72411644-b47ab800-37ae-11ea-9b32-91fa316c066f.png)\n",
    "\n",
    "## Putting it all together\n",
    "LSTM with the 4 gates: There is a **FORGET GATE** which takes _the long term memory_ and forgets part of it. The **LEARN GATE** puts _the short term memory together with the event_ as _the information_ we've recently learned. The **REMEMBER GATE** _joins the long term memory that we haven't yet forgotten plus the new information we've learned in order to update our long term memory_ and output it. And finally, the **USE GATE** also takes _the information we just learned together with long term memory we haven't yet forgotten_, and it uses it to make a prediction and update the short term memory.\n",
    "![image](https://user-images.githubusercontent.com/46597294/72412094-ce68ca80-37af-11ea-848d-7b932d45154a.png)\n",
    "<br /> This is an arbitrary construction. And as many things in machine learning, the reason why it is like this is because it works.\n",
    "## Gated Recurrent Unit (GRU)\n",
    "It combines the forget and the learn gate into an update gate and then runs this through a combine gate. It only returns one working memory instead of a pair of long and short term memories, but it actually seems to work in practice very well too.\n",
    "![image](https://user-images.githubusercontent.com/46597294/72412391-8e561780-37b0-11ea-8a03-8f92500086ef.png)\n",
    "\n",
    "## Peephole Connections\n",
    "Let's remember the forget gate. The forget factor $f_t$ was calculating using as input a combination of short term memory and the event. But what about the long term memory? It seems like we left it away from the decision. Why does a long term memory not have a say into which things get remembered or not? Let's fix that. Let's also **connect the long term memory into the neural network that calculates the forget factor.** Mathematically this just means the input matrix is larger since we're also concatenating it with the long term memory matrix. This is called a peephole connection since now **the long term memory has more access into the decisions made inside the LSTM.** We can do this for every one of the forget-type nodes, and this is what we get: an LSTM with peephole connections.\n",
    "![image](https://user-images.githubusercontent.com/46597294/72412701-59969000-37b1-11ea-8449-aa5e6143eabe.png)\n",
    "![image](https://user-images.githubusercontent.com/46597294/72412732-7632c800-37b1-11ea-9cc7-9a9f0cfbb98c.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
