{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration-Exploitation Dilemma\n",
    "This is a very important concept in RL. While there are some widely adopted techniques for balancing these two competing requirements, this question has inspired an entire subfield of RL and remains an area of active research.<br />\n",
    "### The Setting\n",
    "The RL framework is characterized by an `agent` learned to interact with its environment. We assume that time evolves in discrete timesteps. At the initial timestep, the agent observes the `environment`. You can think of this `observation` as a situation that the environment presents to the agent. Then, it must select an appropriate `action` in response. Then, at the next timestep in response to the agents action, the environment presents a new situation(observation) to the agent. At the same time the environment gives the agent a `reward` which provides some indication of whether the agent has responded appropriately to the environment. Then the process continues where at each timestep, the environment sends the agent an obervation and reward. And in response, the agent must choose an action. And instead of referring to the agent as receiving an observation, it recieves the environment `state`.<br />\n",
    "![image](https://user-images.githubusercontent.com/46597294/73223565-e37b2b80-41a9-11ea-8b67-9c59ab481a3d.png)<br />Whereas the agent interacts with the environment, this interaction is manifest as a sequence of states, actions, and rewards. That said, the reward will always be the most relevant quantity to the agent. To be specific, **any agent has the goal to maximize expected cumulative reward or the sum of the rewards attained over all timesteps**  <br /><div style=\"text-align:center\"> $S_0 \\;\\;  A_0\\;\\;$ <span style=\"color:blue\"> $R_1$ </span> $ \\;\\;  S_1 \\;\\;  A_1 \\;\\; $<span style=\"color:blue\"> $R_2 $ </span>$ \\;\\;  S_2  \\;\\; A_2 \\;\\;  $<span style=\"color:blue\"> $R_3 $ </span>$ ... $</div> <br />In other words, it seeks to find the strategy for choosing actions with the cumulative reward is likely to be quite high. And the agent can only accomplish this by interacting with the environment. This is because at every timestep, the environment decides how much reward the agent receives. In other words, the agent must play by the rules of the environment. But through interaction, the agent can learn those rules and choose appropriate actions to accomplish its goal.\n",
    "\n",
    "### Episodic vs. Continuing Tasks\n",
    "* Episodic Task : Interaction ends at some time step $T$ - Teaching an agent to play a game, then the interaction ends when the agent wins or loses<br /><br />\n",
    "* Continuing Task : Interaction continues without limit - An algorithm that buys and sells stocks in response to the financial market would be best modeled as an agent in the continuing task\n",
    "        \n",
    "        \n",
    "### The Reward Hypothesis\n",
    "All goals can be framed as the maximization of **expected** cumulative reward.\n",
    "### Goals and Rewards\n",
    "$r = min(v_x, v_{max}) - 0.005(v_y^2+v_z^2)-0.05y^2 - 0.02 {\\lvert}u{\\rvert}^2 +0.02$ <br /><br />This line is pulled from the appendix of the DeepMind paper and describes how the reward is decided at every time step. They frame the problem as an episodic task where if the human falls, then the episode is terminated. \n",
    "<br />\n",
    "* **0.02(Constant reward for not falling)** - To see this, first note that if the robot falls, the episode terminates. And that's a missed opportunity to collect more of this positive reward. And in general, if the robot walks for ten time steps, that's only 10 opportunities to get reward. And if it stays walking for 100, that's a lot more time to collect more reward. So if we get the reward in this way, the agent will try to keep from falling for as long as possible. <br /><br />\n",
    "+ **$min(v_x, v_{max})$(Proportional to the robot's forward velocity)** - Since the reward is proportional to the forward velocity, this will ensure the robot also feels pressured to walk as quickly as possible in the direction of the walking track.<br /><br />\n",
    "+ **0.02${\\lvert}u{\\rvert}^2$(Penalize torques)** - It makes sense to penalize the agent for applying too much force to the joints. This is because otherwise, we could end up with a situation where the humanoid walks too erratically. By penalizing large forces, we can try to keep the movements more smooth and elegant.<br /><br />\n",
    "+ **$- 0.005(v_y^2+v_z^2)-0.05y^2 $(Penalize deviation from forward direction and from center of track** - We want to keep the agent on the track and moving forward. Otherwise, who knows where it could end up walking off too.<br /><br />\n",
    "![image](https://user-images.githubusercontent.com/46597294/73225856-6f448600-41b1-11ea-83a2-47fdac7ae686.png)<br />\n",
    "\n",
    "Google DeepMind demonstrated that from this very simple reward function, the agent is able to learn how to walk in a very human like fashion.\n",
    "\n",
    "### Cumulative Reward \n",
    "Could the agent just maximize the reward in each time step? No, I think a long answer would be alot more satisfying. Actions have short and long term consequences and the agent needs to gain some understanding of the complex effects its actions have on the environment. Along in the walking robot example, the agent always has reward at all time steps in mind, it will learn to choose movement designed for long term stability. How exactly does it keep all time steps in mind? Well, if we're looking at some time step $t$, it's important to note that the rewards for all previous time steps have already been decided as they're in the past. We refer to the sum of rewards from the next timestep onward as the return and denote it with a capital $G$. **The <u>return at time step $t$</u> is** $G_t = R_{t+1}+R_{t+2}+R_{t+3}+R_{t+4}+...$ And at an arbitrary time step, the agent will always choose an action towards the goal of maximizing the return. But it's actually more accurate to say that the agent seeks to maximize expected return.\n",
    "### Discounted Return\n",
    "We will use 'return' and **'discounted return'** interchangably. For an arbitrary time step $t$, both refer to $G_t = R_{t+1}+\\gamma R_{t+2}+\\gamma^2R_{t+3}+... = \\sum_{k=0}^\\infty \\gamma^kR_{t+k+1}$, where $\\gamma \\in [0,1]$. By discounted, we mean that we'll change the goal to care more about _immediate rewards_ rather than rewards that are received further in the future. And it's important to note that discounting is particularly relevant to continuing tasks.<br />\n",
    "* How exactly might you set the value of $ \\gamma $(discount rate)? <br />\n",
    "\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/46597294/73241398-3a522680-41e5-11ea-93d9-a27616c56a54.png)\n",
    "\n",
    "### MDPs\n",
    "Let's learn all about how to rigorously define a reinforcement learning problem as **Markov Decision Process(MDP)**.\n",
    "* **What are the <u>actions</u>?**\n",
    "> We refer to the set of possible actions as the action space and it's common to denote it with a $ \\mathcal{A} $. \n",
    "*  **What are the <u>states</u>?**\n",
    "> The states are just the context provided to the agent for making intelligent actions. We refer to the set of possible states as the state space and it's common to denote with a $ \\mathcal{S} $\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/46597294/73242308-fc0a3680-41e7-11ea-8bfa-03c0bd2b1bf0.png)\n",
    "\n",
    "This picture completely characterizes one method that the environment could use to decide the next state in reward at any point in time.\n",
    "\n",
    "#### Notes\n",
    "---\n",
    "In general, the state space  $ \\mathcal{S} $ is the set of all nonterminal states. In continuing tasks (like the recycling task detailed in the video), this is equivalent to the set of all states. In episodic tasks, we use $ \\mathcal{S}^+ $ to refer to the set of all states, including terminal states. The action space $ \\mathcal{A} $ is the set of possible actions available to the agent. In the event that there are some states where only a subset of the actions are available, we can also use $ \\mathcal{A}(s) $ to refer to the set of actions available in state $ s\\in\\mathcal{S} $.\n",
    "\n",
    "### One-Step Dynamics\n",
    "At an arbitrary time step $t$, the agent-environment interaction has evolved as a sequence of states, actions, and rewards\n",
    "\n",
    "($S_0, A_0, R_1, S_1, A_1, \\ldots, R_{t-1}, S_{t-1}, A_{t-1}, R_t, S_t, A_t$)\n",
    "\n",
    "When the environment responds to the agent at time step $t+1$, it considers only the state and action at the previous time step ($S_t, A_t$) In particular, it does not care what state was presented to the agent more than one step prior. (In other words, the environment does not consider any of {${ S_0, \\ldots, S_{t-1}}$}\n",
    "And, it does not look at the actions that the agent took prior to the last one. ( *In other words* , the environment does not consider any of {${A_0, \\ldots, A_{t-1}}$}\n",
    " Furthermore, how well the agent is doing, or how much reward it is collecting, has no effect on how the environment chooses to respond to the agent. (*In other words*, the environment does not consider any of {$ R_0, \\ldots, R_t \\$}\n",
    "\n",
    "Because of this, we can completely define how the environment decides the state and reward by specifying\n",
    "\n",
    "$ p(s',r|s,a) \\doteq \\mathbb{P} (S_{t+1}=s', R_{t+1}=r|S_t = s, A_t=a) $ \n",
    "\n",
    "for each possible $s', r, s, \\text{and } a$. These conditional probabilities are said to specify the `one-step dynamics of the environment`.\n",
    "#### A (finite) Markov Decision Process (MDP) is defined by:\n",
    "\n",
    "* a (finite) set of states $\\mathcal{S}$\n",
    "* a (finite) set of actions $\\mathcal{A}$\n",
    "* a (finite) set of rewards $\\mathcal{R}$\n",
    "* the one-step dynamics of the environment\n",
    "    $ p(s',r|s,a) \\doteq \\mathbb{P} (S_{t+1}=s', R_{t+1}=r|S_t = s, A_t=a) $ for all $s,s',a $ and $ r$\n",
    "* a discount rate $\\gamma \\in [0,1]$\n",
    "\n",
    "### Policies\n",
    "The simplest kind of policy is a mapping from the set of environment states to the set of possible actions. $\\pi : S -> A$ \n",
    "* A **deterministic policy** is a mapping $\\pi:S->A$ \n",
    "\n",
    "    Income is the state, outcome is the action to take. <br /><br />\n",
    "\n",
    "* A **stochastic policy** is a mapping $\\pi:S\\times A -> [0,1]$ \n",
    "\n",
    "    The stochastic policy will allow the agent to choose actions randomly. <br />$\\pi(a|s) = \\mathcal{P}(A_t = a|S_t = s)$ We define a stochastic policy as a mapping that accepts an environment state S and action A and returns the *probability* that the agent takes action A while in state S.\n",
    "    \n",
    "![image](https://user-images.githubusercontent.com/46597294/73407887-cd9e6f80-433d-11ea-99f3-98ee8ef28900.png)\n",
    "\n",
    "### State-Value Functions\n",
    "We call $v_{\\pi}$ the **state_value function** for policy $\\pi$. The value of state s under a policy $\\pi$ is $v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t|S_t = s] $. The state value function will always correspond to a particular policy  \n",
    "![image](https://user-images.githubusercontent.com/46597294/73409285-2ff96f00-4342-11ea-964e-4babfcb4a529.png)\n",
    "\n",
    "### Bellman Equations\n",
    "The Bellman expectation equation where for a general MDP we have to calculate the *expected* value of the sum. This is because, in general, in more complicated worlds, the immediate reward and next state cannot be known with certainty. The main idea is that we can express the value of any state in the MDP in terms of the immediate reward and the discounted value of the state that follows. In this case, where the reward $r$ and next state $s'$ are drawn from a (conditional) probability distribution $p(s',r|s,a)$, the **Bellman Expectation Equation (for $v_{\\pi}$)** expresses the value of any state $s$ in terms of the *expected* immediate reward and the *expected* value of the next state:\n",
    "![image](https://user-images.githubusercontent.com/46597294/73409659-418f4680-4343-11ea-9233-6bc65eb420c6.png)\n",
    "\n",
    "#### Calculating the Expectation\n",
    "---\n",
    "In the event that the agent's policy ${\\pi}$ is deterministic, the agent selects *action $\\pi(s)$* when in state ss, and the Bellman Expectation Equation can be rewritten as the sum over two variables ($s'$and $r$)\n",
    "\n",
    "$ v_\\pi(s) = \\text{} \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R}}p(s',r|s,\\pi(s))(r+\\gamma v_\\pi(s')) $\n",
    "\n",
    "If the agent's policy $\\pi$ is stochastic, the agent selects action $a$ with *probability $\\pi(a|s)$* when in state $s$, and the Bellman Expectation Equation can be rewritten as the sum over three variables ($s',r$, and $a$)\n",
    "\n",
    "$v_\\pi(s) = \\text{} \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R},a\\in\\mathcal{A}(s)}\\pi(a|s)p(s',r|s,a)(r+\\gamma v_\\pi(s'))$\n",
    "\n",
    "#### There are  more Bellman Equations!\n",
    "> All of the Bellman equations attest to the fact that value functions satisfy recursive relationships.\n",
    "### Optimality\n",
    "* $\\pi' \\geqq \\pi$ if and only if $v_{\\pi'}(s) \\geqq v_\\pi(s)$ for all $s \\in S$ (Note: It is often possible to find two policies that cannot be compared.)\n",
    "* An optimal policy $\\pi_*$ satisfies $\\pi_* \\geqq \\pi$ for all $\\pi$ (Note: An optimal policy is guaranteed to exist, but may not be unique. It's the solution to the MDP and the best strategy to accomplish it's goal.)\n",
    "* The optimal state-value function is denoted $v_*$\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/46597294/73419286-eae53500-4361-11ea-9ccf-c60d6c71f7fe.png)\n",
    "\n",
    "### Action-Value Functions\n",
    "We call $q_\\pi$ the **action-value function for policy $\\pi$.** The value taking action $a$ in state $s$ under a policy $\\pi$ is $q_\\pi(s,a) = \\mathbb{E}_\\pi[G_t|S_t = s, A_t = a]$ <br /> While the state values are a function of the environment state, the action values are a function of the environment state and the agent's action. For each state $s$ and action $a$, the action value function yields the expected discounted return if the agent starts in state $s$ then chooses action $a$ and then uses a policy to choose its action for all future time steps. \n",
    "\n",
    "For a deterministic policy $\\pi$, $v_\\pi (s) = q_\\pi(s,\\pi(s))$ <br />\n",
    "The value of the state-action pair $s, \\pi(s)$ is the expected return if the agent starts in state $s$, takes action $\\pi(s)$ and henceforth follows the policy $\\pi$. In other words, it is the expected return if the agent starts in state $s$, and then follows the policy $\\pi$, which is exactly equal to the value of state $s$.\n",
    "\n",
    "### Optimal Policies\n",
    "If the agent has the optimal action value function, it can quickly obtain an optimal policy, which is the solution to the MDP that we're looking for. This brings us to the question of how the agent could find the optimal value function. <br />\n",
    "Once the agent determines the optimal action-value function $q_*$, it can quickly obtain an optimal policy $\\pi_*$ by setting $\\pi_*(s) = argmax_{a\\in\\mathcal{A}(s}q_*(s,a)$\n",
    "![image](https://user-images.githubusercontent.com/46597294/73422112-42d46980-436b-11ea-963d-7828305b1d06.png)\n",
    "\n",
    "### Monte Carlo Methods\n",
    "To truly understand the environment, the agent needs more episodes! \n",
    "* Reason 1 : The agent hasn't attempted each action from each state.\n",
    "* Reason 2 : The environment's dynamics are stochastic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### A (finite) Markov Decision Process (MDP) is defined by:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
