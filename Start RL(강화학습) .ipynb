{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 강화학습(Reinforcement Learning) 기본개념"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습과 관련된 수식 속 함축적인 의미에 대해 잘 알지 못하면 강화학습을 직관적으로 이해하기에 한계가 있다.<br /> **강화학습은 에이전트가 의사 결정을 위해 사용하는 프레임워크다.** 구체화된 에이전트가 어쩌면 강화학습을 완전히 인식하고 활용하는 가장 좋은 방법일 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에이전트는 **환경(environment)** 내에 위치. 그 환경에는 부분적으로 혹은 완전히 관측 가능한 **상태(state)** 가 있다. 에이전트는 자신의 환경과 상호작용하기 위해 사용할 수 있는 **행동(action)** 집합을 갖는다. 행동의 결과는 그 환경을 새로운 상태로 전이시킨다. 에이전트는 행동을 실행한 후 그에 대응하는 스칼라 **보상(reward)** 를 받는다. 에이전트의 목적은 어떤 상태가 주어졌을 때 어떤 행동을 취할지 결정할 **정책(policy)** 을 학습함으로써 미래에 받을 누적된 보상을 최대화하는 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화의 핵심은 바로 보상을 얻게 해주는 행동의 빈도 증가.\n",
    "강화는 행동심리학에서 연구됐던 분야지만 사실 우리에게 친숙한 개념이다. 사람이 학습하는 과정과 가장 유사한 형태의 알고리즘이라고 생각하면 될 듯. 아기가 첫걸음을 떼는 과정에서 아이는 걷는 법을 다른 사람으로부터 배우지는 않지만 이리저리 시도해 보다 우연히 걷게 되면\n",
    "자신이 하는 행동과 걷게 된다는 보상 사이의 연관관계를 몰라 다시 넘어진다. 하지만 시간이 지남에 따라 그 관계를 잘 학습해서 결국 잘 걷습니다. 이렇게 사람(과 동물)에게 학습의 기본이 되는 **강화**라는 개념이 강화학습의 모티프가 된 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP(Markov Decision Process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습은 결정을 순차적으로 내려야 하는 문제에 적용됩니다. 순차적으로 행동을 결정하는 문제를 정의할 때 사용하는 방법이 `MDP(Markov Decision Process)`입니다. MDP는 순차적 행동 결정 문제를 수학적으로 정의해서 에이전트가 순차적 행동 결정 문제에 접근할 수 있게한다.<br />\n",
    "> Markov Decision Process은 미래 상태를 예측할 때 과거에 있던 모든 일에 대한 정보를 고려하는 것이 아니라 오직 현재상태만을 고려하는 Markov Property에서 따온 이름이다. 아래 그림에서 볼 수 있듯 MDP는 $t+1$의 상태($s^{'}$)를 예측할 때, $t(=s)$ 시간의 상태만을 고려함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "순차적 행동 결정 문제의 구성 요소\n",
    "+ 상태 - 에이전트의 상태, 정적인 요소 뿐만 아니라 에이전트가 움직이는 속도와 같은 동적인 요소도 상태에 포함\n",
    "+ 행동 - 에이전트가 어떠한 상태에서 취할 수 있는 행동, 에이전트가 행동을 취하면 환경은 에이전트에게 보상을 주고 다음 상태를 알려줌\n",
    "+ 보상 - 가장 핵심적인 요소, 강화학습의 목표는 시간에 따라 얻는 보상들의 합을 최대로 하는 정책($\\pi$)을 찾는 것, 보상은 에이전트에 속하지 않는 환경의 일부이고 에이전트는 어떤 상황에서 얼마의 보상이 나오는 지 미리 알지 못함\n",
    "+ 정책 - 제일 좋은 정책은 최적 정책(optimal policy)라고 하며 이 때 보상의 합을 최대로 받을 수 있다. 정책이란 순차적 행동 결정 문제의 _처음 부터 끝까지 모든 상태에 대한 에이전트의 행동_ 을 말함.\n",
    "![image](https://user-images.githubusercontent.com/46597294/72660406-b5ab1f80-3a10-11ea-9d2e-2e96aee0ec9d.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP는 _**상태, 행동, 보상함수, 상태 변환 환률(State Transition Probability), 감가율(Discount Factor)**_ 로 구성돼 있다.<br />\n",
    "문제의 정의는 에이전트가 학습하는 데 가장 중요한 단계 중 하나 입니다. 순차적으로 행동을 결정하는 문제에 대한 정의가 바로 MDP입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**상태 변환 확률** 은 상태 s에서 행동 a를 취했을 때 다른 상태 s'에 도달할 확률입니다. 이 값은 보상과 마찬가지로 에이전트가 알지 못하는 값으로서 에이전트가 아닌 환경의 일부. 상태 변환 확률은 환경의 모델이라고도 부름. 환경은 에이전트가 행동을 취하면 상태 변환 확률을 통해 다음에 에이전트가 갈 상태를 알려줌."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**감가율**은 시간에 따라서 감가하는 비율을 말하며 $ \\gamma $ 로 표기함. 감가율 $ \\gamma $ 는 0과 1 사이의 값! 따라서 보상에 곱해지면 보상이 감소. 만약 현재의 시간 t로 시간 k가 지난 후에 보상을 $ R_{t+k} $ 를 받을 것이라고 하면 그 보상의 가치는 현재로부터 시간이 k만큼 지났기 때문에 미래에 받을 보상 $ R_{t+k} $ 는 $ \\gamma^{k-1} $ 만큼 감가됨.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**정책**은 모든 상태에서 에이전트가 할 행동. 상태가 입력으로 들어오면 행동을 출력으로 내보내는 일종의 함수라고 생각가능<br />\n",
    "$ \\pi(a|s) = P[A_t = a|S_t = s] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가치함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 에이전트가 학습할 수 있도록 문제를 MDP로 정의했다. 에이전트는 MDP를 통해 최적 정책을 찾으면 된다. 어떻게 찾을 수 있을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 반환값의 정의(감가율을 적용한 보상들의 합)<br />\n",
    "$ G_b = R_{t+1} + \\gamma R_{t+2} + \\gamma^2R_{t+3} + \\gamma^3R_{t+4} ... $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 가치함수<br />\n",
    "$ v(s) = E[G_t | S_t = s] $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `상태가치함수(State-Value Function)` - 현재 상태의 좋고/나쁨을 표현<br />\n",
    "$ v(s) = E[R_{t+1} + \\gamma R_{t+2} +\\gamma^2R_{t+3} ... | S_t = s] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `행동 가치 함수(Action-Value Function)` - 현재 행동의 좋고 나쁨을 표현 <br />\n",
    "$Q_{\\pi}(s,a) = E_{\\pi}[R_{t+1} + \\gamma R_{t+2} +\\gamma^2R_{t+3} ... | S_t = s,A_t = a]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상태 가치 함수와 행동 가치 함수를 통해서 알아내고자하는 최종 목표는 최적의 정책 ${\\pi}^{*}$ 를 구하는 것이다. 보상이 최대화되는 최적의 정책을 알아내는 방법은 크게 2가지가 있는데, _상태 가치 함수를 이용_ 해서 알아내는 방법을 **Planning** 이라고 하고 _행동 가치 함수를 이용_ 해서 알아내는 방법을 **강화학습(RL)** 이라 한다. Planning을 이용하려면 환경에 대한 모델 정보(Transition Prob, $P(s,s^{'})$와 이에 대한 보상 $R(s,s^{'})$)를 알고 있어야 한다. 하지만, 강화학습 방법은 모델에 대한 정보 없어도 에이전트가 주어진 환경에서 행동을 취하고 얻은 실제 경험을 통해 학습을 진행할 수 있다. 강화학습에서 적절한 _행동 가치 함수값을 알아내기 위한 구체적인 알고리즘_ 으로는 _SARSA, Q-Learning(off-policy TD control algorithm), Policy Gradient_ 등이 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${\\pi}^{*} = argmax_a Q(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 수식을 통해 반복적으로 $Q(s_t, a_t)$를 업데이트 해나갈시  $Q(s_t, a_t)$가 최적의 행동가치함수  $Q^{*}(s_t, a_t)$에 수렴한다는 사실은 수학적으로 증명되었다. <br /><br />\n",
    "$ Q(s,a) = r + r max_{a^{'}}Q(s^{'},a{'}) $ <br /><br />\n",
    "$s^{'},a{'}$ 는 각각 다음 상태와 행동을 말한다. 위 방정식은 Q-러닝 알고리즘의 핵심인 **벨만 방정식** 이라고 한다.\n",
    "에이전트는 환경의 역학에 대해 모른 채로 행동 a를 시도하고 보상 r과 다음 상태 $s^{'}$ 형태로 어떤 일이 일어나는지 관측한다. $max_{a^{'}}Q(s^{'},a{'})$ 는 다음 상태에 대해 최대 Q값을 주는 논리적 행동을 선택한다. 방정식의 모든 항을 알고 있으면 그 현재 상태-행동 쌍을 위한 Q값이 업데이트된다. 반복적으로 업데이트를 수행하다 보면 결국 Q함수를 학습할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/46597294/72660994-b6e04a80-3a18-11ea-9a24-f6ddc9bf6315.png)\n",
    "\n",
    "Q-러닝은 'off-policy' RL알고리즘이다. 이 알고리즘은 해당 정책으로부터 경험을 직접 샘플링하지 않고도 정책 개선을 위해 학습한다. 즉, Q값은 에이전트에 의해 사용된 기반 정책과 독립적으로 학습 된다 <br />\n",
    "* on policy - 지금 행동으로 수집한 데이터를 이용해서 실시간으로 갱신하고 버림. (e.g. Policy Gradient) \n",
    "* off policy - 지금 행동으로 수집한 데이터를 파라미터 갱신에 쓰지 않음. <br />\n",
    "\n",
    "### $\\epsilon$-Greedy\n",
    "<br />Q-Learning 및 강화학습 적용할 때, 보통 입실론-그리디 라는 기법을 적용한다. 에이전트가 항상 최대 Q값으로 행동을 하게 되면 데이터 수집과정에서 다양성이 감소하고 이로 인해 학습결과가 Local Optima 에 빠질 확률이 높아진다. 따라서 $\\epsilon$-Greedy 기법은 에이전트가 $\\epsilon$ 확률로 최적의 행동이 아닌 랜덤한 행동(Exploration)을 하게 되고, $1-\\epsilon$의 확률로 최적의 행동(Exploitation)을 하게 함으로써 에이전트가 다양한 상태를 경험하고 수집할 수 있게 하고 이는 곧 더 좋은 성능으로 이어진다. 하지만 계속해서 랜덤한 행동을 할 수 없기 때문에 학습 초반에는 $\\epsilon$값을 크게 잡았다가 학습이 진행될 수록 점차적으로 감소시켜준다.($\\epsilon$결정하는 문제를 Exploration-Exploitation Tradeoff라고 하고, 강화학습의 고전적인 난제 중 하나)\n",
    "<br />\n",
    "### DQN (Deep-Q-Networks) <br />\n",
    "알파고로 유명한 DeepMind사에서 창안한 강화학습을 위한 인공신경망 구조로써 기존의 Q-Networks와 딥러닝을 결합한 기법\n",
    "* 기존의 Q-Networks은 얕은 층의 ANN구조 사용했지만, DQN은 깊은 층의 CNN구조 사용\n",
    "* 인간의 해마<sup>Hipocampus</sup>에서 영감 받아 **리플레이 메모리<sup>Replay Memory</sup>** 라는 기법 사용하여 효율적인 강화학습이 가능하게 만들었다.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/46597294/72661082-8cdb5800-3a19-11ea-816d-fc811d65a2bd.png)\n",
    "\n",
    "### Double Q-Learning\n",
    "<br />Q-Learning 업데이트 공식을 보면 다음 state에서 취할 수 있는 action에 대한 Q값 들 중에서 max값을 취하는 부분이 있다. 여기서 의도치 않게 특정 Q값을 과대평가(overestimation)할 위험성이 생기게 된다. 처음에는 과대평가로 인한 오차가 크지 않더라도, 계속 Q값을 업데이트 하다보면 심각한 오차로 이어지게 된다. **Double Q-Learning** 은 이런 과대평가로 인한 오차를 최소화하기 위해 만들어졌다. 기존의 Q 업데이트 공식에서 다음 state에서 취할 수 있는 action 중 가장 높은 행동가치 함수를 가진 action을 선택하는 부분은 계속해서 파라미터가 업데이트 되는 **Online Networks** 이용해서 예측한 Q값을 사용하고 선택된 해당 action으로 추정한 Q값으로 갱신할 때는 파라미터가 실시간으로 업데이트 되지 않고 **주기적으로 Online Networks의 파라미터를 복사하는 Target Networks를 이용해서 추정한 Q값 이용** 해서 Q값을 업데이트 한다. 두 개의 Q값을 이용한다는 의미에서 Double Q-Learning이라고 불리게 됨.<br /><br />\n",
    "$Y_t^{DoubleQ} = R_{t+1}+\\gamma Q(S_{t+1}, argmax_aQ(S_{t+1},a;\\theta_t);\\theta^{'}_t)$\n",
    "\n",
    "### Dueling Q-Learning\n",
    "<br /> Duel Q-Learning은 상태 가치 함수와 행동 가치 함수를 분리해서 학습하는 형태로 네트워크를 구성하는 기법이다. 기본 DQN의 경우, Action과 무관한 State의 가치를 제대로 추정할 수 없는 문제점이 있다. State value function은 당장의 action과 무관하게 상태의 좋고, 나쁜 정도를 평가할 수 있다. 따라서 더 정확한 state value function을 학습하기 위해 Duel DQN은 **Neural Network의 마지막 부분에서 State value function을 추정하는 부분과 Advantage를 추정하는 부분을 분리**. Duel DQN의 마지막에 분리되어 학습된 State value function 과 Advantage function의 예측값은 Advantage function의 평균만큼 빼서 normalize 한 뒤, State value funciton과 더해져 다시 합쳐지게 된다. 수식으로 나타내면 <br /><br />\n",
    "$Q(s,a;\\theta, \\alpha, \\beta) = V(s;\\theta, \\beta) + \\Big(A(s,a;\\theta,\\alpha) - {1\\over \\lvert \\mathrm{A} \\rvert}\\Sigma_{a^{'}}A(s,a^{'};\\theta,\\alpha)\\Big)$\n",
    "![image](https://user-images.githubusercontent.com/46597294/72661033-22c2b300-3a19-11ea-839d-b902dacb96ac.png)\n",
    "\n",
    "### Prioritized Experience Replay(PER)\n",
    "<br />DQN구조 보면 Experience Replay에 MDP형태로 표현되는 샘플들 저장하고, 순차적으로 수집되는 데이터가 아니라 Experience Replay에서 랜덤하게 샘플을 뽑아서 학습을 진행할 경우, 학습 데이터 수집 과정에서 발생하는 correlation 문제를 안정적으로 DQN이 학습할 수 있다는 걸 알 수 있다. 하지만 이렇게 랜덤하게 Experience Replay에서 데이터를 뽑아 학습을 할 경우, 학습에 도움이 많이 되는 샘플들이 뽑힐 수도 있고, 도움이 1도 안되는 샘플이 많이 뽑힐 수도 있다. PER은 이런 문제를 해결하기 위해 **우선순위(Priority)** 를 이용해서 학습에 도움이 많이 되는 샘플들이 많이 뽑힐 수 있게 만드는 방법론. 이때 우선순위로 사용하는 값은 **TD_Error(Temporal Difference Error) $\\delta$** 이다. $\\delta$를 수식으로 나타내면 <br /><br />\n",
    "$\\delta_i = r_t + \\gamma max_{a \\in A}Q_{\\theta}(s_{t+1},a) - Q_{\\theta}(s_t,a_t)$ <br /><br />\n",
    "TD-Error $\\delta$가 큰 샘플일수록 Q 값 업데이트 과정에서 파라미터 갱신이 많이 일어난다. 하지만 우선순위를 이용할 경우, 학습을 위해 뽑히는 샘플들의 다양성이 감소하는 문제가 있어 **Importance Sampling Weight**를 지정해준다. 우선순위 보정에서 쓰이는 hyperparameter $\\alpha, \\beta$ 는 자세히 적어놓지 않겠다. 나중에..\n",
    "## Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source from \n",
    "\n",
    "# Solaris님\n",
    "# 고급 딥러닝 알고리즘 \n",
    "# 텐서플로& 케라스 강화학습 \n",
    "# etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
